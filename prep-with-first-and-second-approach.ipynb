{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e23c21",
   "metadata": {},
   "source": [
    "# TurkishSpamFilterEnhancement-NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e663be5",
   "metadata": {},
   "source": [
    "# Let's Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d41ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faab68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading CSV file as pandas Dataframe\n",
    "df = pd.read_csv(\"archive/trspam.csv\", header=None, encoding=\"utf-8\", on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a852c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sayın Yetkili ,\\n  Gelişen ve değişen günümüz ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sayın Yetkili,\\n \\n28 Kasım 2010 tarihli KPSS ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sayın Yetkili ,\\n  Gelişen ve değişen günümüz ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T.C.\\nURLA KAYMAKAMLIĞI\\nURLA HAKAN ÇEKEN ANAD...</td>\n",
       "      <td>spam</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Ã¶zlem sent you a message.\\n\\n(no subject)\\n\\n...</td>\n",
       "      <td>ham</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Sayın Hocalarım, değerli zamanınızı aldığım iç...</td>\n",
       "      <td>ham</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>PGh0bWw+DQo8aGVhZD4NCjx0aXRsZT5DaGlwJmFtcDtQSU...</td>\n",
       "      <td>ham</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>PCFET0NUWVBFIGh0bWwgUFVCTElDICItLy9XM0MvL0RURC...</td>\n",
       "      <td>ham</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>703 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0     1      2\n",
       "0                                                  0.0   0.0    NaN\n",
       "1    Sayın Yetkili ,\\n  Gelişen ve değişen günümüz ...  spam    NaN\n",
       "2    Sayın Yetkili,\\n \\n28 Kasım 2010 tarihli KPSS ...  spam    NaN\n",
       "3    Sayın Yetkili ,\\n  Gelişen ve değişen günümüz ...  spam    NaN\n",
       "4    T.C.\\nURLA KAYMAKAMLIĞI\\nURLA HAKAN ÇEKEN ANAD...  spam    NaN\n",
       "..                                                 ...   ...    ...\n",
       "698  Ã¶zlem sent you a message.\\n\\n(no subject)\\n\\n...   ham    NaN\n",
       "699  Sayın Hocalarım, değerli zamanınızı aldığım iç...   ham    NaN\n",
       "700  PGh0bWw+DQo8aGVhZD4NCjx0aXRsZT5DaGlwJmFtcDtQSU...   ham    NaN\n",
       "701  PCFET0NUWVBFIGh0bWwgUFVCTElDICItLy9XM0MvL0RURC...   ham    NaN\n",
       "702                                                NaN   NaN  496.0\n",
       "\n",
       "[703 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad711f4",
   "metadata": {},
   "source": [
    "# Dataset Cleaning and Reshaping Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25efba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing na values from dataframe\n",
    "def dt_na_value_cleaning(data):\n",
    "    print(\"\\nData Shape : \", data.shape)\n",
    "    print(\"\\nNull values before removal:: \")\n",
    "    print(data.isna().sum())\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "    data.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    print(\"\\nNull values after removal: \")\n",
    "    print(data.isna().sum())\n",
    "    print(\"\\nData Shape after cleaning :\" , data.shape)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e06c578",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Removing duplicate values\n",
    "def duplicate_content_removal(dt, col, ini_row):\n",
    "    dt = dt.iloc[1: , :]\n",
    "    print(\"\\nNumber of data before removing duplicates: \",ini_row)\n",
    "    duplicate_count = dt[col].duplicated().sum()\n",
    "    print(\"\\nNumber of Duplicates: \", duplicate_count)\n",
    "    \n",
    "    description_data = dt[col].drop_duplicates()\n",
    "    cleaned_row = len(description_data)\n",
    " \n",
    "    if (ini_row - cleaned_row) > 0:\n",
    "        print(\"\\nTotal data reduction : \", (ini_row - cleaned_row))\n",
    "        print(\"\\nNumber of data after removing duplicates is :\", cleaned_row)\n",
    "    else:\n",
    "        print(\"\\nNo duplicate data.\")\n",
    "    return list(description_data)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9efcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 703 entries, 0 to 702\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   E_Mail  701 non-null    object \n",
      " 1   Label   702 non-null    object \n",
      " 2   NaN     1 non-null      float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 16.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df=df.rename(columns={0: \"E_Mail\", 1: \"Label\", 2:\"NaN\"})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea73687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape :  (702, 2)\n",
      "\n",
      "Null values before removal:: \n",
      "E_Mail    2\n",
      "Label     1\n",
      "dtype: int64\n",
      "\n",
      "Null values after removal: \n",
      "E_Mail    0\n",
      "Label     0\n",
      "dtype: int64\n",
      "\n",
      "Data Shape after cleaning : (700, 2)\n",
      "\n",
      "Number of data before removing duplicates:  700\n",
      "\n",
      "Number of Duplicates:  66\n",
      "\n",
      "Total data reduction :  67\n",
      "\n",
      "Number of data after removing duplicates is : 633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     0.594286\n",
       "spam    0.405714\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deleting the unnecessary columns and rows\n",
    "del df[\"NaN\"]\n",
    "df = df.iloc[1: , :]\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "df = dt_na_value_cleaning(df)\n",
    "E_Mail = duplicate_content_removal(df, 'E_Mail', df.shape[\n",
    "0])\n",
    "df.shape\n",
    "df['Label'].value_counts(normalize=True)# Removing duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745713c",
   "metadata": {},
   "source": [
    "# Splitting Dataset into Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e31bb6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 2)\n",
      "(140, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     0.614286\n",
       "spam    0.385714\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomize the dataset\n",
    "data_randomized = df.sample(frac=1, random_state=1)\n",
    "# Calculate index for split\n",
    "training_test_index = round(len(data_randomized) * 0.8)\n",
    "# Split into training and test sets\n",
    "training_set = data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_set = data_randomized[training_test_index:].reset_index(drop=True)\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)\n",
    "training_set['Label'].value_counts(normalize=True)\n",
    "test_set['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d4739",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f6bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import string\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java, isJVMStarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7400d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function for Turkish letters, to fix upper case - lower case issue\n",
    "def trlower(metin):\n",
    "    def trlower_(harf):\n",
    "        if harf=='I': sonuc = 'ı'\n",
    "        elif harf=='İ': sonuc = 'i'\n",
    "        else: sonuc = harf.lower()\n",
    "        return sonuc\n",
    "    sonuc = ''\n",
    "    for a in metin:\n",
    "        sonuc += trlower_(a)\n",
    "    return sonuc\n",
    "\n",
    "#Function for removing digits\n",
    "def sayi(metin): \n",
    "    sonuc = ''.join([i for i in metin if not i.isdigit()])\n",
    "    return sonuc\n",
    "\n",
    "#Function for removing punctuation\n",
    "def noktalama(metin):\n",
    "    sonuc = \"\".join([i for i in metin if i not in string.punctuation])\n",
    "    return sonuc\n",
    "\n",
    "#Function for removing white-spaces\n",
    "def wspace(metin):\n",
    "    if metin is None:\n",
    "        sonuc=''\n",
    "    sonuc=metin.strip()\n",
    "    return sonuc\n",
    "\n",
    "#Function for removing words less than 3 characters\n",
    "def remove_length(x):\n",
    "    res = list()\n",
    "    for word in x:\n",
    "        if len(word) >= 3:\n",
    "            res.append(word)\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c641997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ZEMBEREK_PATH = 'libs/zemberek-full.jar'\n",
    "\n",
    "if os.path.isfile(ZEMBEREK_PATH):\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    print(\"File does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cce2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEMBEREK_PATH = 'libs/zemberek-full.jar'\n",
    "import jpype\n",
    "# Check if JVM is already running\n",
    "if not jpype.isJVMStarted():\n",
    "    # Start the JVM\n",
    "    startJVM(jpype.getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % ZEMBEREK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e96b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    morphology = TurkishMorphology.createWithDefaults()\n",
    "    analysis: java.util.ArrayList = (morphology.analyzeAndDisambiguate(text).bestAnalysis())\n",
    "    pos: List[str] = []\n",
    "    \n",
    "    for i, analysis in enumerate(analysis, start=1):\n",
    "        f'\\nAnalysis {i}: {analysis}',\n",
    "        f'\\nPrimary POS {i}: {analysis.getPos()}'\n",
    "        f'\\nPrimary POS (Short Form) {i}: {analysis.getPos().shortForm}'\n",
    "        \n",
    "        pos.append(f'{str(analysis.getLemmas()[0])}') \n",
    "    return \" \".join(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc79fdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bahra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33c2eb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_Mail</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hafta genel  haziran sayı küçük işle siber s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>html head meta    meta      head body   table ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hoca ben matematik ortalama bütünle gir ol gen...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>merhaba    veri  site tekrar  durum     ilgi  ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from           başbakan   mar from    muhteşem...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              E_Mail Label\n",
       "0    hafta genel  haziran sayı küçük işle siber s...   ham\n",
       "1  html head meta    meta      head body   table ...   ham\n",
       "2  hoca ben matematik ortalama bütünle gir ol gen...   ham\n",
       "3  merhaba    veri  site tekrar  durum     ilgi  ...   ham\n",
       "4  from           başbakan   mar from    muhteşem...   ham"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying cleaning functions to training data set one at a time to check the results\n",
    "training_set[\"E_Mail\"] = training_set[\"E_Mail\"].apply(trlower)\n",
    "training_set[\"E_Mail\"] = training_set[\"E_Mail\"].apply(sayi)\n",
    "training_set[\"E_Mail\"] = training_set[\"E_Mail\"].apply(noktalama)\n",
    "training_set[\"E_Mail\"] = training_set[\"E_Mail\"].apply(wspace)\n",
    "\n",
    "#Removing Turkish stopwords with NLTK library\n",
    "training_set['E_Mail'] = training_set['E_Mail'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "training_set['E_Mail'] = training_set['E_Mail'].str.split().apply(remove_length)\n",
    "\n",
    "training_set[\"E_Mail\"] = training_set[\"E_Mail\"].apply(lemmatizer)\n",
    "#since ZEMBEREK lemmatizer gives UNK as output \n",
    "# if a word cannot be processed, replacing it with ‘’\n",
    "\n",
    "training_set[\"E_Mail\"] = training_set[\"E_Mail\"].str.replace(\"UNK\", '')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e601a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_Mail</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sayın yetkili geliş değiş gün teknoloji çalış ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sayın yetkili kasım tarih kpss sınav sonuç yap...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sayın yetkili geliş değiş gün teknoloji çalış ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urla kaymakam urla haka çek anadol lise müdür ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sayın yetkili hız büyü geliş ol toplum büyü uy...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              E_Mail Label\n",
       "0  sayın yetkili geliş değiş gün teknoloji çalış ...  spam\n",
       "1  sayın yetkili kasım tarih kpss sınav sonuç yap...  spam\n",
       "2  sayın yetkili geliş değiş gün teknoloji çalış ...  spam\n",
       "3  urla kaymakam urla haka çek anadol lise müdür ...  spam\n",
       "4  sayın yetkili hız büyü geliş ol toplum büyü uy...  spam"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying cleaning functions to training data set one at a time to check the results\n",
    "df[\"E_Mail\"] = df[\"E_Mail\"].apply(trlower)\n",
    "df[\"E_Mail\"] = df[\"E_Mail\"].apply(sayi)\n",
    "df[\"E_Mail\"] = df[\"E_Mail\"].apply(noktalama)\n",
    "df[\"E_Mail\"] = df[\"E_Mail\"].apply(wspace)\n",
    "\n",
    "#Removing Turkish stopwords with NLTK library\n",
    "df['E_Mail'] = df['E_Mail'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "df['E_Mail'] = df['E_Mail'].str.split().apply(remove_length)\n",
    "\n",
    "df[\"E_Mail\"] = df[\"E_Mail\"].apply(lemmatizer)\n",
    "#since ZEMBEREK lemmatizer gives UNK as output \n",
    "# if a word cannot be processed, replacing it with ‘’\n",
    "\n",
    "df[\"E_Mail\"] = df[\"E_Mail\"].str.replace(\"UNK\", '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying cleaning functions to training data set one at a time to check the results\n",
    "test_set[\"E_Mail\"] = test_set[\"E_Mail\"].apply(trlower)\n",
    "test_set[\"E_Mail\"] = test_set[\"E_Mail\"].apply(sayi)\n",
    "test_set[\"E_Mail\"] = test_set[\"E_Mail\"].apply(noktalama)\n",
    "test_set[\"E_Mail\"] = test_set[\"E_Mail\"].apply(wspace)\n",
    "\n",
    "#Removing Turkish stopwords with NLTK library\n",
    "test_set['E_Mail'] = test_set['E_Mail'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "test_set['E_Mail'] = test_set['E_Mail'].str.split().apply(remove_length)\n",
    "\n",
    "test_set[\"E_Mail\"] = test_set[\"E_Mail\"].apply(lemmatizer)\n",
    "#since ZEMBEREK lemmatizer gives UNK as output \n",
    "# if a word cannot be processed, replacing it with ‘’\n",
    "\n",
    "test_set[\"E_Mail\"] = test_set[\"E_Mail\"].str.replace(\"UNK\", '')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a97a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_csv('train_p.csv',sep=',',index=False);\n",
    "test_set.to_csv('test_p.csv',sep=',',index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724be82c",
   "metadata": {},
   "source": [
    "# Classification (First Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7ac5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6773"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the vocabulary\n",
    "training_set['E_Mail'] = training_set['E_Mail'].str.split()\n",
    "vocabulary = []\n",
    "for email in training_set['E_Mail']:\n",
    "    for word in email:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = list(set(vocabulary))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9428ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_Mail</th>\n",
       "      <th>Label</th>\n",
       "      <th>redmond</th>\n",
       "      <th>denizli</th>\n",
       "      <th>muallim</th>\n",
       "      <th>çekik</th>\n",
       "      <th>mesih</th>\n",
       "      <th>görev</th>\n",
       "      <th>imza</th>\n",
       "      <th>zam</th>\n",
       "      <th>...</th>\n",
       "      <th>kazan</th>\n",
       "      <th>sein</th>\n",
       "      <th>irade</th>\n",
       "      <th>bayramoğlu</th>\n",
       "      <th>saldırgan</th>\n",
       "      <th>cümleten</th>\n",
       "      <th>bina</th>\n",
       "      <th>kiremit</th>\n",
       "      <th>sözkonusu</th>\n",
       "      <th>yanaz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[hafta, genel, haziran, sayı, küçük, işle, sib...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[html, head, meta, meta, head, body, table, ta...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hoca, ben, matematik, ortalama, bütünle, gir,...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[merhaba, veri, site, tekrar, durum, ilgi, aya...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[from, başbakan, mar, from, muhteşem, bir, mek...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6775 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              E_Mail Label  redmond  denizli  \\\n",
       "0  [hafta, genel, haziran, sayı, küçük, işle, sib...   ham        0        0   \n",
       "1  [html, head, meta, meta, head, body, table, ta...   ham        0        0   \n",
       "2  [hoca, ben, matematik, ortalama, bütünle, gir,...   ham        0        0   \n",
       "3  [merhaba, veri, site, tekrar, durum, ilgi, aya...   ham        0        0   \n",
       "4  [from, başbakan, mar, from, muhteşem, bir, mek...   ham        0        0   \n",
       "\n",
       "   muallim  çekik  mesih  görev  imza  zam  ...  kazan  sein  irade  \\\n",
       "0        0      0      0      0     0    0  ...      0     0      0   \n",
       "1        0      0      0      0     0    0  ...      0     0      0   \n",
       "2        0      0      0      0     0    0  ...      0     0      0   \n",
       "3        0      0      0      0     0    0  ...      0     0      0   \n",
       "4        0      0      0      0     0    0  ...      0     0      0   \n",
       "\n",
       "   bayramoğlu  saldırgan  cümleten  bina  kiremit  sözkonusu  yanaz  \n",
       "0           0          0         0     0        0          0      0  \n",
       "1           0          0         0     0        0          0      0  \n",
       "2           0          0         0     0        0          0      0  \n",
       "3           0          0         0     0        0          0      0  \n",
       "4           0          1         0     0        0          0      0  \n",
       "\n",
       "[5 rows x 6775 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dataframe to calculate probabilities on\n",
    "word_counts_per_email = {unique_word: [0] * len(training_set['E_Mail']) for unique_word in vocabulary}\n",
    "for index, email in enumerate(training_set['E_Mail']):\n",
    "    for word in email:\n",
    "        word_counts_per_email[word][index] += 1\n",
    "word_counts = pd.DataFrame(word_counts_per_email)\n",
    "word_counts.head()\n",
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e17ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolating spam and ham messages first\n",
    "spam_emails = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_emails = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_emails) / len(training_set_clean)\n",
    "p_ham = len(ham_emails) / len(training_set_clean)\n",
    "# N_Spam\n",
    "n_words_per_spam_emails = spam_emails['E_Mail'].apply(len)\n",
    "n_spam = n_words_per_spam_emails.sum()\n",
    "# N_Ham\n",
    "n_words_per_ham_emails = ham_emails['E_Mail'].apply(len)\n",
    "n_ham = n_words_per_ham_emails.sum()\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1\n",
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_emails[word].sum() # spam_messages already defined\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    n_word_given_ham = ham_emails[word].sum() # ham_messages already defined\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham+ alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e6e85",
   "metadata": {},
   "source": [
    "### Applying bayes theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "046afdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01b2a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(email):\n",
    "    email = re.sub('\\W', ' ', email )\n",
    "    email = email.lower().split()\n",
    "    \n",
    "    p_spam_given_email = p_spam\n",
    "    p_ham_given_email = p_ham\n",
    "    \n",
    "    for word in email:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_email *= parameters_spam[word]\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_email *= parameters_ham[word]\n",
    "        print('P(Spam|email):', p_spam_given_email)\n",
    "        print('P(Ham|email):', p_ham_given_email)\n",
    "    if p_ham_given_email > p_spam_given_email:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_email < p_spam_given_email:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Label: Ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deefcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(email):\n",
    "    email = re.sub('\\W', ' ', email )\n",
    "    email = email.lower().split()\n",
    "    \n",
    "    p_spam_given_email = p_spam\n",
    "    p_ham_given_email = p_ham\n",
    "    \n",
    "    for word in email:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_email*= parameters_spam[word]\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_email *= parameters_ham[word]\n",
    "        \n",
    "    if p_ham_given_email > p_spam_given_email:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_email > p_ham_given_email:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "356d37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['predicted'] = test_set['E_Mail'].apply(classify_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a325f4c",
   "metadata": {},
   "source": [
    "### Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6788a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 104\n",
      "Incorrect: 36\n",
      "Accuracy: 0.7428571428571429\n",
      "False Negative Ratio: 22.857142857142858\n",
      "False Positive Ratio: 2.857142857142857\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Metrics\n",
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)\n",
    "\n",
    "#False Positive and False Negative Metrics\n",
    "fp=0\n",
    "fn=0\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == 'spam':\n",
    "        if row['predicted']=='ham':\n",
    "            fn+=1\n",
    "    if row['Label'] == 'ham':\n",
    "        if row['predicted']=='spam':\n",
    "            fp+=1\n",
    "print('False Negative Ratio:', fn/total*100)\n",
    "print('False Positive Ratio:', fp/total*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e8b33",
   "metadata": {},
   "source": [
    "## Classification (Second Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "933cfefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bahra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bahra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "82\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from typing import List\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a50f2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(df['E_Mail'])\n",
    "label = pd.DataFrame(df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59896715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text data into vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df['E_Mail'])\n",
    "vectors.shape\n",
    "#features = word_vectors\n",
    "features = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb28ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df['Label'], test_size=0.15, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08850334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn packages for building classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c5168fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVC': SVC(gamma=1.0, kernel='sigmoid'),\n",
       " 'KN': KNeighborsClassifier(n_neighbors=49),\n",
       " 'NB': MultinomialNB(alpha=0.2),\n",
       " 'DT': DecisionTreeClassifier(min_samples_split=7, random_state=111),\n",
       " 'LR': LogisticRegression(penalty='l1', solver='liblinear'),\n",
       " 'RF': RandomForestClassifier(n_estimators=31, random_state=111)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize multiple classification models\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier(n_neighbors=49)\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=31, random_state=111)\n",
    "#create a dictionary of variables and models\n",
    "clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}\n",
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a656e52b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m clfs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     10\u001b[0m     train(v, X_train, y_train)\n\u001b[1;32m---> 11\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     pred_scores_word_vectors\u001b[38;5;241m.\u001b[39mappend((k, [accuracy_score(y_test , pred)]))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#getting the accuracy scores of the classifiers using tf\u0002idf vectorizing\u001b[39;00m\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(clf, features)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(clf, features):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\neighbors\\_base.py:824\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    817\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    820\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    821\u001b[0m     )\n\u001b[0;32m    822\u001b[0m )\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 824\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    836\u001b[0m ):\n\u001b[0;32m    837\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    838\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    839\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:277\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    290\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    291\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    298\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:95\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\fixes.py:151\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreadpoolctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_threadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[43m_ThreadpoolInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m                          \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:373\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_modules_with_enum_process_module_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dl_iterate_phdr()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:485\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m         filepath \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[1;32m--> 485\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_module_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m     kernel_32\u001b[38;5;241m.\u001b[39mCloseHandle(h_process)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[0;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[1;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[1;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[1;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[1;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#fit the data onto the models\n",
    "def train(clf, features, targets):\n",
    "    clf.fit(features, targets)\n",
    "\n",
    "def predict(clf, features):\n",
    "    return (clf.predict(features))\n",
    "\n",
    "pred_scores_word_vectors = []\n",
    "for k,v in clfs.items():\n",
    "    train(v, X_train, y_train)\n",
    "    pred = predict(v, X_test)\n",
    "    pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)]))\n",
    "\n",
    "#getting the accuracy scores of the classifiers using tf\u0002idf vectorizing\n",
    "pred_scores_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddfc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
